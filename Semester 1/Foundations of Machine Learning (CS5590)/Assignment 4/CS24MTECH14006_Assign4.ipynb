{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights after 1 iteration of gradient descent are  [-1.00316626  1.50535086  0.50196867]\n",
      "Updated Logistic Regression Model after 1 iteration is given by \n",
      "fθ(x1, x2) = -1.0032 + 1.5054 * x1 + 0.5020 * x2\n",
      "\n",
      "Weights at convergence are  [-3.80988747  5.36228896  1.63108539]\n",
      "Accuracy: 0.6666666666666666\n",
      "Precision_model: 0.6\n",
      "Recall_model: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Solution 5) Logisticss Regresssion \n",
    "import numpy as nump  # Importing the library for numerical operations\n",
    "\n",
    "# **Solution for Question 5) a- Implementing L R model\n",
    "\n",
    "# Dataset: Training & Test Sets\n",
    "# Create training & test data\n",
    "data_training = nump.array([[0.346, 0.780, 0],  # Training data for predict_with_thing labels\n",
    "                       [0.303, 0.439, 0],  \n",
    "                       [0.358, 0.729, 0],  \n",
    "                       [0.602, 0.863, 1],  \n",
    "                       [0.790, 0.753, 1],  \n",
    "                       [0.611, 0.965, 1]])\n",
    "data_testing = nump.array([[0.959, 0.382, 0],   # Testing data\n",
    "                      [0.750, 0.306, 0],   \n",
    "                      [0.395, 0.760, 0],   \n",
    "                      [0.823, 0.764, 1],   \n",
    "                      [0.761, 0.874, 1],   \n",
    "                      [0.844, 0.435, 1]])\n",
    "\n",
    "# Separate the features (X) & labels (y) for train & test sets\n",
    "X_training_feature = data_training[:, :2]  # Getting the features (x1, x2) for training data\n",
    "y_training_label = data_training[:, 2]   # Get y labels for train\n",
    "X_testing_feature = data_testing[:, :2]    # Features in test are selecting here\n",
    "y_testing_label = data_testing[:, 2]     # Using this as last column in testing data\n",
    "\n",
    "# Initializing model parameters \n",
    "theta_model_para = nump.array([-1.0, 1.5, 0.5])  # Here I am Using starting weights given in question\n",
    "rate_learn_lr = 0.1  # Set the value of learning rate given in question\n",
    "\n",
    "# Logistic function (Sigmoid_lr) for calculating probability\n",
    "def sigmoid_lr(z):\n",
    "    return 1 / (1 + nump.exp(-z))  # Equation for sigmoid curve\n",
    "\n",
    "# Cross entropy loss function for Computing error\n",
    "def error_cross_entropy(y, y_pred):\n",
    "    # Returning mean loss for y & predict_with_thed\n",
    "    return -nump.mean(y * nump.log(y_pred) + (1 - y) * nump.log(1 - y_pred))\n",
    "\n",
    "# Computing gradients for the weights θ\n",
    "def computing_gradient(X, y, theta_model_para):\n",
    "    length_dataset = len(y)  # Here I am calculating Length of the dataset (samples)\n",
    "    feature_with_bias = nump.c_[nump.ones(X.shape[0]), X]  # Add the bias to features\n",
    "    predicts = sigmoid_lr(nump.dot(feature_with_bias, theta_model_para))  # Getting predictions\n",
    "    error = predicts - y  # Calculate error by substracting\n",
    "    gradient = (1 / length_dataset) * nump.dot(feature_with_bias.T, error)  # Taking the gradient\n",
    "    return gradient  # Return gradient result\n",
    "\n",
    "# Function to train the logistic regression model with gradient descent\n",
    "def train_l_r_with_g_d(X, y, theta_model_para, rate_learn_lr, iterations=1000, tolerance=1e-6):\n",
    "    feature_with_bias = nump.c_[nump.ones(X.shape[0]), X]  # Adding bias to X\n",
    "    for _ in range(iterations):  # Loop for each step\n",
    "        gradient = computing_gradient(X, y, theta_model_para)  # Call the function for gradient\n",
    "        theta_model_para -= rate_learn_lr * gradient  # Subtracting gradient to update weights\n",
    "        compute_loss = error_cross_entropy(y, sigmoid_lr(nump.dot(feature_with_bias, theta_model_para)))  # Compute loss\n",
    "        if nump.linalg.norm(gradient) < tolerance:  # Checking for convergence using gradient\n",
    "            break  # Stop if converged\n",
    "    return theta_model_para  # Return final weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# **Solution for Question 5) b)2 ** Perform One Iteration of Gradient Descent\n",
    "gradient_one_iterat = computing_gradient(X_training_feature, y_training_label, theta_model_para)  # Get gradient for one step\n",
    "theta_model_para_one_iter = theta_model_para - rate_learn_lr * gradient_one_iterat  # Updating the weights\n",
    "print(\"Weights after 1 iteration of gradient descent are \", theta_model_para_one_iter)\n",
    "\n",
    "# Print down updated logistic regression model\n",
    "updated_model_l_r = f\"fθ(x1, x2) = {theta_model_para_one_iter[0]:.4f} + {theta_model_para_one_iter[1]:.4f} * x1 + {theta_model_para_one_iter[2]:.4f} * x2\"\n",
    "print(\"Updated Logistic Regression Model after 1 iteration is given by \")\n",
    "print(updated_model_l_r)  # Showing final equation\n",
    "\n",
    "\n",
    "\n",
    "# **Solution for Question 5) b)3 ** Training Model until it Converges\n",
    "theta_model_para_converged = train_l_r_with_g_d(X_training_feature, y_training_label, theta_model_para, rate_learn_lr)  # Calling train\n",
    "print(\"\\nWeights at convergence are \", theta_model_para_converged)\n",
    "\n",
    "\n",
    "def predict_with_th(X, theta_model_para, threshold=0.5):  # Predic function taking threshold\n",
    "    feature_with_bias = nump.c_[nump.ones(X.shape[0]), X]  # Bias addition\n",
    "    probability = sigmoid_lr(nump.dot(feature_with_bias, theta_model_para))  # Sigmoid call for probability\n",
    "    return (probability >= threshold).astype(int)  # Convert to binary output\n",
    "\n",
    "# Test dataset predictions\n",
    "y_pred_test = predict_with_th(X_testing_feature, theta_model_para_converged)  # Final predictions for test\n",
    "\n",
    "# Calculating evaluation metrics: accu, precision, & recall\n",
    "def metrics_calculation(y_true, y_pred):\n",
    "    true_positive = nump.sum((y_true == 1) & (y_pred == 1))  # Calculate true positives\n",
    "    false_positive = nump.sum((y_true == 0) & (y_pred == 1))  # Falsely predicted as positives\n",
    "    true_negative = nump.sum((y_true == 0) & (y_pred == 0))  # True negatives part\n",
    "    false_neagative = nump.sum((y_true == 1) & (y_pred == 0))  # False negatives part\n",
    "    accu_model = (true_positive + true_negative) / len(y_true)  # Dividing for accu_model\n",
    "    precision_model = true_positive / (true_positive + false_positive) if true_positive + false_positive > 0 else 0  # Handling no division by zero\n",
    "    recall_model = true_positive / (true_positive + false_neagative) if true_positive + false_neagative > 0 else 0  # Compute recall correctly\n",
    "    return accu_model, precision_model, recall_model  # Returning all results\n",
    "\n",
    "# Evaluating on test data\n",
    "accu_model, precision_model, recall_model = metrics_calculation(y_testing_label, y_pred_test)\n",
    "\n",
    "# Display final metrics of the trained model\n",
    "print(\"Accuracy:\", accu_model)\n",
    "print(\"Precision_model:\", precision_model)\n",
    "print(\"Recall_model:\", recall_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 6) Kaggle - Taxi Fare Prediction\n",
    "#  Importing required libraries for using\n",
    "import pandas as pd  # Using pandas for handling of data\n",
    "import numpy as np  # Using numpy for numerical operations\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV  # Importing train test split & the grid search CV\n",
    "from sklearn.ensemble import GradientBoostingRegressor  # Using the Gradient boosting regressor for training of data\n",
    "from sklearn.metrics import mean_squared_error  # Importing M S E for checking model performance\n",
    "from sklearn.preprocessing import StandardScaler  # This is for doing data scaling\n",
    "from math import radians, cos, sin, asin, sqrt  # Importing this for distance calculating\n",
    "\n",
    "# Here i am defining haversine_distance_cal function for calculation of distances\n",
    "def haversine_distance_cal(longi_1, latti_1, longi_2, latti_2):\n",
    "    # I am mapping values to radians, because haversine_distance_cal formula need radians\n",
    "    longi_1, latti_1, longi_2, latti_2 = map(radians, [longi_1, latti_1, longi_2, latti_2])\n",
    "    \n",
    "    # Calculating the difference between longitude and latitude\n",
    "    diff_longi = longi_2 - longi_1\n",
    "    diff_latti = latti_2 - latti_1\n",
    "    \n",
    "    # Applying haversine_distance_cal formula\n",
    "    a = sin(diff_latti / 2) ** 2 + cos(latti_1) * cos(latti_2) * sin(diff_longi / 2) ** 2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    rad = 6371  # Radius of earth in kilometer\n",
    "    \n",
    "    return c * rad  # Returning the distance\n",
    "\n",
    "# As dataset is much large , I am Loading train data with only 90,000 rows \n",
    "taxi_training_data_set_nyc = pd.read_csv('train.csv', nrows=90000)\n",
    "\n",
    "# Filter rows with fare in the valid range\n",
    "# If fare amount is less than 2.6 dollar or greater than 310 dollar then it will be dropped, considering situation of new york city\n",
    "taxi_training_data_set_nyc = taxi_training_data_set_nyc[(taxi_training_data_set_nyc['fare_amount'] >= 2.6) & (taxi_training_data_set_nyc['fare_amount'] <= 310)]\n",
    "\n",
    "# Filtering passengers count in reasonable range, allowing only less than 5 passenger\n",
    "taxi_training_data_set_nyc = taxi_training_data_set_nyc[(taxi_training_data_set_nyc['passenger_count'] >= 0) & (taxi_training_data_set_nyc['passenger_count'] <= 5)]\n",
    "\n",
    "# Cleaning invalid latitude and longitude from data\n",
    "# he range of latitude is from -90° to 90°, so only this range is allowed\n",
    "# The range of longitude is -180° to +180°, so only this range is allowed\n",
    "taxi_training_data_set_nyc = taxi_training_data_set_nyc[(taxi_training_data_set_nyc['pickup_latitude'] >= -90) & (taxi_training_data_set_nyc['pickup_latitude'] <= 90)]\n",
    "taxi_training_data_set_nyc = taxi_training_data_set_nyc[(taxi_training_data_set_nyc['pickup_longitude'] >= -180) & (taxi_training_data_set_nyc['pickup_longitude'] <= 180)]\n",
    "taxi_training_data_set_nyc = taxi_training_data_set_nyc[(taxi_training_data_set_nyc['dropoff_latitude'] >= -90) & (taxi_training_data_set_nyc['dropoff_latitude'] <= 90)]\n",
    "taxi_training_data_set_nyc = taxi_training_data_set_nyc[(taxi_training_data_set_nyc['dropoff_longitude'] >= -180) & (taxi_training_data_set_nyc['dropoff_longitude'] <= 180)]\n",
    "\n",
    "# Convert pickup datetime to datetime object & dropping bad rows\n",
    "taxi_training_data_set_nyc['pickup_datetime'] = pd.to_datetime(taxi_training_data_set_nyc['pickup_datetime'], errors='coerce')\n",
    "taxi_training_data_set_nyc = taxi_training_data_set_nyc.dropna(subset=['pickup_datetime'])  # This line is to remove invalid rows\n",
    "\n",
    "# Extracting time features from pickup datetime\n",
    "taxi_training_data_set_nyc['hour'] = taxi_training_data_set_nyc['pickup_datetime'].dt.hour  # Getting hour\n",
    "taxi_training_data_set_nyc['day'] = taxi_training_data_set_nyc['pickup_datetime'].dt.day  # Getting day\n",
    "taxi_training_data_set_nyc['month'] = taxi_training_data_set_nyc['pickup_datetime'].dt.month  # Getting month\n",
    "taxi_training_data_set_nyc['year'] = taxi_training_data_set_nyc['pickup_datetime'].dt.year  # Getting year\n",
    "\n",
    "# Calculating distance for all  the rows\n",
    "taxi_training_data_set_nyc['distance'] = taxi_training_data_set_nyc.apply(lambda row: haversine_distance_cal(\n",
    "    row['pickup_longitude'], row['pickup_latitude'],\n",
    "    row['dropoff_longitude'], row['dropoff_latitude']), axis=1)\n",
    "\n",
    "# Now Drop unused columns for cleaning data\n",
    "taxi_training_data_set_nyc = taxi_training_data_set_nyc.drop(columns=['key', 'pickup_datetime'])\n",
    "\n",
    "# Initializing  the scaler & scaling numerical columns\n",
    "scaler = StandardScaler()  # Using StandardScaler for normalizing\n",
    "features = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'distance']\n",
    "taxi_training_data_set_nyc[features] = scaler.fit_transform(taxi_training_data_set_nyc[features])\n",
    "\n",
    "# Splitting train data into features & target\n",
    "X_feature = taxi_training_data_set_nyc.drop(columns=['fare_amount'])  # Features are being extracted\n",
    "y_target = taxi_training_data_set_nyc['fare_amount']  # Target column is selected\n",
    "\n",
    "# Dividing data into train & validation\n",
    "X_training_feature, X_val_feature, y_training_label, y_val_label = train_test_split(X_feature, y_target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Defining hyperparameter grid for tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50,100,200,300],  # No. of trees to testing\n",
    "    'learning_rate': [0.05, 0.1, 0.2],  # Testing diff learning rate\n",
    "    'max_depth': [3, 5, 7],  # Trying different max depths\n",
    "    'subsample': [0.8, 1.0],  # Varying sub sample rates\n",
    "    'min_samples_split': [2, 5, 10]  # Testing mini sample splits\n",
    "}\n",
    "\n",
    "# Initialize the Gradient Boosting Regressor for training\n",
    "g_b_r = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Performing grid search for finding best parameters\n",
    "grid_search = GridSearchCV(estimator=g_b_r, param_grid=param_grid, scoring='neg_mean_squared_error', cv=3, n_jobs=-1)\n",
    "grid_search.fit(X_training_feature, y_training_label)\n",
    "\n",
    "# Getting best model after grid searching\n",
    "best_g_b_r = grid_search.best_estimator_\n",
    "print(\"Best Parameters are - \", grid_search.best_params_)\n",
    "\n",
    "# Predicte on validation data for checking performance\n",
    "y_pred = best_g_b_r.predict(X_val_feature)\n",
    "rmse = np.sqrt(mean_squared_error(y_val_label, y_pred))  # Calculating RMSE\n",
    "print(f\"Validation RMSE is  {rmse}\")\n",
    "\n",
    "# Load test data for final prediction\n",
    "testing_data_nyc = pd.read_csv('test.csv')\n",
    "\n",
    "# Keeping 'key' column for submission file\n",
    "testing_keys = testing_data_nyc['key']\n",
    "\n",
    "# Feature engineering in test data\n",
    "testing_data_nyc['pickup_datetime'] = pd.to_datetime(testing_data_nyc['pickup_datetime'], errors='coerce')  # Converting datetime\n",
    "testing_data_nyc = testing_data_nyc.dropna(subset=['pickup_datetime'])  # Drop invalid datetime rows\n",
    "testing_data_nyc['hour'] = testing_data_nyc['pickup_datetime'].dt.hour  # Extracting hour\n",
    "testing_data_nyc['day'] = testing_data_nyc['pickup_datetime'].dt.day  # Extracting day\n",
    "testing_data_nyc['month'] = testing_data_nyc['pickup_datetime'].dt.month  # Extracting month\n",
    "testing_data_nyc['year'] = testing_data_nyc['pickup_datetime'].dt.year  # Extracte year\n",
    "testing_data_nyc['distance'] = testing_data_nyc.apply(lambda row: haversine_distance_cal(\n",
    "    row['pickup_longitude'], row['pickup_latitude'],\n",
    "    row['dropoff_longitude'], row['dropoff_latitude']), axis=1)\n",
    "\n",
    "# Normalizing test data features\n",
    "testing_data_nyc[features] = scaler.transform(testing_data_nyc[features])\n",
    "\n",
    "# Dropping unused columns and predicting fare amounts\n",
    "testing_features = testing_data_nyc.drop(columns=['key', 'pickup_datetime'])\n",
    "testing_data_nyc['fare_amount'] = best_g_b_r.predict(testing_features)\n",
    "\n",
    "# Preparing final submission file\n",
    "submission = pd.DataFrame({\n",
    "    'key': testing_keys,  # Adding key column for submission\n",
    "    'fare_amount': testing_data_nyc['fare_amount']  # Adding predicted fare\n",
    "})\n",
    "\n",
    "# Saving submission file\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file submission.csv has been created & saved.\") \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
