{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a13e7ba-6888-4a72-adcb-a0c855708b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8868\n"
     ]
    }
   ],
   "source": [
    "#**** Question 3- Decision Tree ****\n",
    "#Solution a)\n",
    "# Step 1- Importing the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Step 2- Loading Dataset\n",
    "#wine_data_set is the URL from which we are loading dataset\n",
    "wine_data_set = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "# Dataset are separated by ;, hence it is delimiter\n",
    "wine_red_quality = pd.read_csv(wine_data_set, delimiter=';')\n",
    "\n",
    "# Step 3- Separating the labels and the features\n",
    "X = wine_red_quality.iloc[:, :-1].values  # ELeven first columns\n",
    "y = wine_red_quality.iloc[:, -1].values   # lst column loaded in y\n",
    "\n",
    "# Binary classification (if quality >= 7 then 1, else 0)\n",
    "y = (y >= 7).astype(int)\n",
    "\n",
    "# Step 4- Function information gain and entropy\n",
    "def entropy(y):\n",
    "    # Number of ccurances of each class \n",
    "    counts_class = np.bincount(y)\n",
    "    # Converting counts to probabilities\n",
    "    prob_class = counts_class/ len(y)\n",
    "    return -np.sum([p * np.log2(p) for p in prob_class if p > 0])\n",
    "\n",
    "def inform_gain(y, left_split_y, right_split_y):\n",
    "    # Finding sample which fall into left split, p is its proportion\n",
    "    p = len(left_split_y) / len(y)\n",
    "    # Inform_gain is diffrence of parents entropy to weighted sum of child entropies\n",
    "    return entropy(y) - p * entropy(left_split_y) - (1 - p) * entropy(right_split_y)\n",
    "\n",
    "# Step 5- Decision Tree (DT) Class implementation\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "    \n",
    "\n",
    "    #Building recursively DT\n",
    "    def fit(self, X, y, depth=0):\n",
    "        number_of_samples, number_of_features = X.shape\n",
    "        if number_of_samples <= 1 or (self.max_depth is not None and depth >= self.max_depth):\n",
    "            return np.bincount(y).argmax()\n",
    "        #Find the best threshold and best feature\n",
    "        best_feature, best_thres = self.best_split(X, y)\n",
    "        if best_feature is None:\n",
    "            return np.bincount(y).argmax()\n",
    "        #determine which go to left child and right child\n",
    "        indices_l = X[:, best_feature] < best_thres\n",
    "        indices_r = X[:, best_feature] >= best_thres\n",
    "\n",
    "        left_sub_tree = self.fit(X[indices_l], y[indices_l], depth + 1)\n",
    "        right_sub_tree = self.fit(X[indices_r], y[indices_r], depth + 1)\n",
    "\n",
    "        return (best_feature, best_thres, left_sub_tree, right_sub_tree)\n",
    "\n",
    "    def best_split(self, X, y):\n",
    "        #Based on inform gain best feature, thres and gain tracked\n",
    "        gain = -1\n",
    "        feature, thres_best = None, None\n",
    "        num_samples, num_features = X.shape\n",
    "\n",
    "        for current_feature in range(num_features):#iterate over feature\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                indices_l = X[:, current_feature] < threshold\n",
    "                indices_r = X[:, current_feature] >= threshold\n",
    "                if len(y[indices_l]) == 0 or len(y[indices_r]) == 0:\n",
    "                    continue\n",
    "\n",
    "                current_gain = inform_gain(y, y[indices_l], y[indices_r])\n",
    "                if current_gain > gain:\n",
    "                    gain = current_gain\n",
    "                    feature = current_feature\n",
    "                    thres_best = threshold\n",
    "\n",
    "        return feature, thres_best # Reutrning best split means best thresthold and feature\n",
    "    # Identify class for each sample in X (done by Predict)\n",
    "    def predict(self, X):\n",
    "        return [self._predict(inputs, self.tree) for inputs in X]\n",
    "    #Recursively traverse tree\n",
    "    def _predict(self, inputs, tree):\n",
    "        if isinstance(tree, (int, np.int64)):\n",
    "            return tree\n",
    "        feature_predict, thres_predict, sub_tree_left, sub_tree_right = tree\n",
    "        if inputs[feature_predict] < thres_predict:\n",
    "            return self._predict(inputs, sub_tree_left)\n",
    "        else:\n",
    "            return self._predict(inputs, sub_tree_right)\n",
    "\n",
    "# Step 6- DT(decision tree) training\n",
    "deci_tree = DecisionTree(max_depth=5) #Initialize D tree with max depth of 5\n",
    "deci_tree.tree = deci_tree.fit(X, y)\n",
    "\n",
    "# Step 7- Using training data predict\n",
    "predicts = deci_tree.predict(X)\n",
    "\n",
    "# Finding the accuracy\n",
    "accuracy_final = np.mean(predicts == y)\n",
    "print(f\"Training Accuracy: {accuracy_final:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afa29c23-2458-423b-8a73-030a1ccbf7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8868\n",
      "Cross-Validation Accuracy: 0.8762\n"
     ]
    }
   ],
   "source": [
    "#**** Question 3- Decision Tree + cross valid ****\n",
    "#Solution b)\n",
    "# Step 1- Importing the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Step 2- Loading Dataset\n",
    "#wine_data_set is the URL from which we are loading dataset\n",
    "wine_data_set = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "# Dataset are separated by ;, hence it is delimiter\n",
    "wine_red_quality = pd.read_csv(wine_data_set, delimiter=';')\n",
    "\n",
    "# Step 3- Separating the labels and the features\n",
    "X = wine_red_quality.iloc[:, :-1].values  # ELeven first columns\n",
    "y = wine_red_quality.iloc[:, -1].values   # lst column loaded in y\n",
    "\n",
    "# Binary classification (if quality >= 7 then 1, else 0)\n",
    "y = (y >= 7).astype(int)\n",
    "\n",
    "# Step 4- Function information gain and entropy\n",
    "def entropy(y):\n",
    "    # Number of ccurances of each class \n",
    "    counts_class = np.bincount(y)\n",
    "    # Converting counts to probabilities\n",
    "    prob_class = counts_class/ len(y)\n",
    "    return -np.sum([p * np.log2(p) for p in prob_class if p > 0])\n",
    "\n",
    "def inform_gain(y, left_split_y, right_split_y):\n",
    "    # Finding sample which fall into left split, p is its proportion\n",
    "    p = len(left_split_y) / len(y)\n",
    "    # Inform_gain is diffrence of parents entropy to weighted sum of child entropies\n",
    "    return entropy(y) - p * entropy(left_split_y) - (1 - p) * entropy(right_split_y)\n",
    "\n",
    "# Step 5- Decision Tree (DT) Class implementation\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "    \n",
    "\n",
    "    #Building recursively DT\n",
    "    def fit(self, X, y, depth=0):\n",
    "        number_of_samples, number_of_features = X.shape\n",
    "        if number_of_samples <= 1 or (self.max_depth is not None and depth >= self.max_depth):\n",
    "            return np.bincount(y).argmax()\n",
    "        #Find the best threshold and best feature\n",
    "        best_feature, best_thres = self.best_split(X, y)\n",
    "        if best_feature is None:\n",
    "            return np.bincount(y).argmax()\n",
    "        #determine which go to left child and right child\n",
    "        indices_l = X[:, best_feature] < best_thres\n",
    "        indices_r = X[:, best_feature] >= best_thres\n",
    "\n",
    "        left_sub_tree = self.fit(X[indices_l], y[indices_l], depth + 1)\n",
    "        right_sub_tree = self.fit(X[indices_r], y[indices_r], depth + 1)\n",
    "\n",
    "        return (best_feature, best_thres, left_sub_tree, right_sub_tree)\n",
    "\n",
    "    def best_split(self, X, y):\n",
    "        #Based on inform gain best feature, thres and gain tracked\n",
    "        gain = -1\n",
    "        feature, thres_best = None, None\n",
    "        num_samples, num_features = X.shape\n",
    "\n",
    "        for current_feature in range(num_features):#iterate over feature\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                indices_l = X[:, current_feature] < threshold\n",
    "                indices_r = X[:, current_feature] >= threshold\n",
    "                if len(y[indices_l]) == 0 or len(y[indices_r]) == 0:\n",
    "                    continue\n",
    "\n",
    "                current_gain = inform_gain(y, y[indices_l], y[indices_r])\n",
    "                if current_gain > gain:\n",
    "                    gain = current_gain\n",
    "                    feature = current_feature\n",
    "                    thres_best = threshold\n",
    "\n",
    "        return feature, thres_best # Reutrning best split means best thresthold and feature\n",
    "    # Identify class for each sample in X (done by Predict)\n",
    "    def predict(self, X):\n",
    "        return [self._predict(inputs, self.tree) for inputs in X]\n",
    "    #Recursively traverse tree\n",
    "    def _predict(self, inputs, tree):\n",
    "        if isinstance(tree, (int, np.int64)):\n",
    "            return tree\n",
    "        feature_predict, thres_predict, sub_tree_left, sub_tree_right = tree\n",
    "        if inputs[feature_predict] < thres_predict:\n",
    "            return self._predict(inputs, sub_tree_left)\n",
    "        else:\n",
    "            return self._predict(inputs, sub_tree_right)\n",
    "\n",
    "# Step 6- DT(decision tree) training\n",
    "deci_tree = DecisionTree(max_depth=5) #Initialize D tree with max depth of 5\n",
    "deci_tree.tree = deci_tree.fit(X, y)\n",
    "\n",
    "# Step 7- Using training data predict\n",
    "predicts = deci_tree.predict(X)\n",
    "\n",
    "# Finding the accuracy\n",
    "accuracy_final = np.mean(predicts == y)\n",
    "print(f\"Training Accuracy: {accuracy_final:.4f}\")\n",
    "\n",
    "# Step 8- Funcntion for trainig tree\n",
    "def tree_train(X_train, y_train):\n",
    "    tree = DecisionTree(max_depth=5)\n",
    "    tree.tree = tree.fit(X_train, y_train)\n",
    "    return tree\n",
    "\n",
    "# Step 7: Define cross validation function\n",
    "def cross_valid_accu(X, y, folds=10):\n",
    "    ten_folds = KFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "    accuracies = []\n",
    "\n",
    "    #For each fold generate indices for testing and training set\n",
    "    for ind_train, ind_test in ten_folds.split(X):\n",
    "        #for features\n",
    "        X_train, X_test = X[ind_train], X[ind_test]\n",
    "        #for labels\n",
    "        y_train, y_test = y[ind_train], y[ind_test]\n",
    "        \n",
    "        model = tree_train(X_train, y_train)\n",
    "        # Now make prediction on test set\n",
    "        predict_cross_valid = model.predict(X_test)\n",
    "        # Calculating accuracy by comparing labels\n",
    "        accu_cross_valid= np.mean(predict_cross_valid == y_test) #Accuracy for each fold is putted here\n",
    "        accuracies.append(accu_cross_valid)\n",
    "    \n",
    "    return np.mean(accuracies)\n",
    "\n",
    "# Perform cross-validation\n",
    "cross_validation_accuracy = cross_valid_accu(X, y)\n",
    "print(f'Cross-Validation Accuracy: {cross_validation_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5053f43-8c84-405f-a78b-2d7b9ab9c791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy without improvement is 0.8868\n",
      "Accuracy with improvement is 1.0000\n"
     ]
    }
   ],
   "source": [
    "#**** Question 3- Decision Tree ****\n",
    "#Solution c)\n",
    "# Step 1- Importing the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "\n",
    "\n",
    "# Step 2- Loading Dataset\n",
    "#wine_data_set is the URL from which we are loading dataset\n",
    "wine_data_set = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "# Dataset are separated by ;, hence it is delimiter\n",
    "wine_red_quality = pd.read_csv(wine_data_set, delimiter=';')\n",
    "\n",
    "# Step 3- Separating the labels and the features\n",
    "X = wine_red_quality.iloc[:, :-1].values  # ELeven first columns\n",
    "y = wine_red_quality.iloc[:, -1].values   # lst column loaded in y\n",
    "\n",
    "# Binary classification (if quality >= 7 then 1, else 0)\n",
    "y = (y >= 7).astype(int)\n",
    "\n",
    "# Step 4- Function information gain and entropy\n",
    "def entropy(y):\n",
    "    # Number of ccurances of each class \n",
    "    counts_class = np.bincount(y)\n",
    "    # Converting counts to probabilities\n",
    "    prob_class = counts_class/ len(y)\n",
    "    return -np.sum([p * np.log2(p) for p in prob_class if p > 0])\n",
    "\n",
    "def inform_gain(y, left_split_y, right_split_y):\n",
    "    # Finding sample which fall into left split, p is its proportion\n",
    "    p = len(left_split_y) / len(y)\n",
    "    # Inform_gain is diffrence of parents entropy to weighted sum of child entropies\n",
    "    return entropy(y) - p * entropy(left_split_y) - (1 - p) * entropy(right_split_y)\n",
    "\n",
    "# Step 5- Decision Tree (DT) Class implementation\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "    \n",
    "\n",
    "    #Building recursively DT\n",
    "    def fit(self, X, y, depth=0):\n",
    "        number_of_samples, number_of_features = X.shape\n",
    "        if number_of_samples <= 1 or (self.max_depth is not None and depth >= self.max_depth):\n",
    "            return np.bincount(y).argmax()\n",
    "        #Find the best threshold and best feature\n",
    "        best_feature, best_thres = self.best_split(X, y)\n",
    "        if best_feature is None:\n",
    "            return np.bincount(y).argmax()\n",
    "        #determine which go to left child and right child\n",
    "        indices_l = X[:, best_feature] < best_thres\n",
    "        indices_r = X[:, best_feature] >= best_thres\n",
    "\n",
    "        left_sub_tree = self.fit(X[indices_l], y[indices_l], depth + 1)\n",
    "        right_sub_tree = self.fit(X[indices_r], y[indices_r], depth + 1)\n",
    "\n",
    "        return (best_feature, best_thres, left_sub_tree, right_sub_tree)\n",
    "\n",
    "    def best_split(self, X, y):\n",
    "        #Based on inform gain best feature, thres and gain tracked\n",
    "        gain = -1\n",
    "        feature, thres_best = None, None\n",
    "        num_samples, num_features = X.shape\n",
    "\n",
    "        for current_feature in range(num_features):#iterate over feature\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                indices_l = X[:, current_feature] < threshold\n",
    "                indices_r = X[:, current_feature] >= threshold\n",
    "                if len(y[indices_l]) == 0 or len(y[indices_r]) == 0:\n",
    "                    continue\n",
    "\n",
    "                current_gain = inform_gain(y, y[indices_l], y[indices_r])\n",
    "                if current_gain > gain:\n",
    "                    gain = current_gain\n",
    "                    feature = current_feature\n",
    "                    thres_best = threshold\n",
    "\n",
    "        return feature, thres_best # Reutrning best split means best thresthold and feature\n",
    "    # Identify class for each sample in X (done by Predict)\n",
    "    def predict(self, X):\n",
    "        return [self._predict(inputs, self.tree) for inputs in X]\n",
    "    #Recursively traverse tree\n",
    "    def _predict(self, inputs, tree):\n",
    "        if isinstance(tree, (int, np.int64)):\n",
    "            return tree\n",
    "        feature_predict, thres_predict, sub_tree_left, sub_tree_right = tree\n",
    "        if inputs[feature_predict] < thres_predict:\n",
    "            return self._predict(inputs, sub_tree_left)\n",
    "        else:\n",
    "            return self._predict(inputs, sub_tree_right)\n",
    "\n",
    "# Step 6- DT(decision tree) training\n",
    "deci_tree = DecisionTree(max_depth=5) #Initialize D tree with max depth of 5\n",
    "deci_tree.tree = deci_tree.fit(X, y)\n",
    "\n",
    "# Step 7- Using training data predict\n",
    "predicts = deci_tree.predict(X)\n",
    "\n",
    "# Finding the accuracy without impovement\n",
    "accuracy_final = np.mean(predicts == y)\n",
    "print(f\"Training Accuracy without improvement is {accuracy_final:.4f}\")\n",
    "\n",
    "##For improvements with grid search (hyperparameter tuning), ensemble method(random forest), feature scaling\n",
    "# Step 8- Scaling the features\n",
    "scal_improve = StandardScaler() #Standardize the features\n",
    "#Compute SD and mean on training data and then scaling\n",
    "X_improve = scal_improve.fit_transform(X)\n",
    "\n",
    "# Step 9- Cross valid function with tuning\n",
    "def r_f_cross_validation_improve(improve_X, improve_y, num_of_fold=10):\n",
    "    # dictionary\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200], #no of trees\n",
    "        'max_depth': [None, 10, 20, 30], # maximum tree depth\n",
    "        'min_samples_split': [2, 5, 10], #min number of samples for spliting\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'criterion': ['gini', 'entropy'] #To measure quality of split\n",
    "    }\n",
    "\n",
    "    state_r_f = RandomForestClassifier(random_state=42)\n",
    "    #Using Random Forest for tuning\n",
    "    search_in_grid = GridSearchCV(estimator=state_r_f, param_grid=param_grid, cv=num_of_fold, n_jobs=-1, scoring='accuracy')\n",
    "    search_in_grid.fit(improve_X, improve_y)\n",
    "   \n",
    "    return search_in_grid.best_estimator_\n",
    "\n",
    "best_model_store = r_f_cross_validation_improve(X, y) #It stores the model which is obtained as best model\n",
    "best_model_store.fit(X, y) #On complete dataset, train best model\n",
    "improved_accu_final = best_model_store.score(X, y) #Finding accuracy of model on dataset\n",
    "print(f'Accuracy with improvement is {improved_accu_final:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9d8dbc-3d13-40a2-a468-7da171bb5a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
